---
title: Forest Information from LiDAR data
author: Chris Reudenbach
date: '2021-11-20'
slug: []
categories: ["aGIS"]
tags: ["forest", "lidar","ecology"]
description: ''
image: '/assets/images/forest-sp.jpg'
toc: true
output:
  blogdown::html_page:
    keep_md: yes
weight: 101

---
## How to get started?


For this first example we take a typical situation:
- we have no idea about the software and possible solutions
- we have no idea about LiDAR data processing and analysis
- we just google something like *lidR package tutorial*

Among the top ten is [The lidR package book](https://jean-romain.github.io/lidRbook/) . So let's follow the white rabbit...

## Creating a Canopy Height Model (CHM) reloaded

After successful application of the tutorial we will transfer it into a suitable script for our workflow. Please check the comments for better understanding and do it **step by step!** again. Please note that the following script is meant to be an basic example how: 
- to organize scripts in common 
- with a hands on example for creating a canopy height model.


After revisiting the tutorial is seems to be a good choice to follow the tutorial of the `lidR` developer that is  [Digital Surface Model and Canopy Height model](https://jean-romain.github.io/lidRbook/chm.html)  in the above tutorial. Why? because for the first Jean-Romain Roussel explains 6 ways how to create in a very simple approach a CHM, second to show up that it makes sense to read and third to loop back because it does not work with bigger files. Let us start with the new script structure.


## Basic Script to derive a Canopy Height Model

<script src="https://gist.github.com/gisma/89881e23f5c2da91d161a9668386b715.js"></script>

      

```{r echo=FALSE}
blogdown::shortcode("hp5", "/gi-modules/assets/misc/chm_one_tile.html","700", "680","center", "Canopy Height Model map")
```

## lidR catalog - coping with computer memory resources

The above example is based on a las-tile of 250 by 250 meter. That means a fairly **small** tile size. If you did not run in memory or CPU problems you can deal with these tiles by simple looping.  

**But** you have to keep in mind that even if a tile based processing can easily be handled with loops but has some pitfalls. E.g. if you compute some focal operations, you have to make sure that if the filter is close to the boundary of the tile that data from the neighboring tile is loaded in addition. Also the tile size may be a problem for your memory availability. 

The `lidR` package comes with a feature called catalog and a set of catalog functions which make tile-based life easier. A catalog meta object also holds information on e.g. the cartographic reference system of the data or the cores used for parallel processing. So we start over again - this time with a **real** data set.

Please note that dealing with `lidR` catalogs is pretty stressful for the memory administration of your `rsession`. So best practices is to **Restart your Rsesseion and clean up your environment**. This will help you to avoid frustrating situation like restarting your PC after hours of waiting...




### lidR catalog script to derive a canopy height model
The following scripts will setup a lidR catalog and a more sophisticated approach to calculate the DEM, DSM and CHM. Please note that you have to exchange the code snippet dealing with the LiDAR data, If not already done download the course related [LiDAR data set of the MOF area](http://gofile.me/3Z8AJ/omf5DvyhL) (VPN required) . Store the the data to the e.g. `envrmt$path_l_raw` folder. 



```{r echo=FALSE} 
blogdown::shortcode("collapse", "Workaround for the IO error problem with the lidR catalog",
                    "Please download and copy the [get_vrt_img.R](https://gist.github.com/gisma/5127a360728666eddb96807ad37bb475) script into the subfolder *functions* of your project src folder.Depending on your installation it might be necessary to work around an unexpected behaviour with the returnd vrt files",
                    .type = "html")
```



<script src="https://gist.github.com/gisma/4172ef049b116abb1454233c8950d587.js"></script>

```{r warmup, include= FALSE,echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE, include=FALSE}
knitr::knit_global()
require(envimaR)

# MANDANTORY: defining the root folder DO NOT change this line
root_folder = "~/edu/agis"

#-- Further customization of the setup by the user this section
#-- can be freely customized only the definition of additional packages
#-- and directory paths MUST be done using the two variables
#-- appendpackagesToLoad and appendProjectDirList
#-- feel free to remove this lines if you do not need them
# define  additional packages comment if not needed
appendpackagesToLoad = c("lidR","future","lwgeom")
# define additional subfolders comment if not needed
appendProjectDirList =  c("data/lidar/","data/lidar/l_raster/","data/lidar/l_raw/","data/lidar/l_norm/")

## define current projection (It is not magic you need to check the meta data or ask your instructor)
## ETRS89 / UTM zone 32N
proj4 = "+proj=utm +zone=32 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
epsg_number = 25832

lidR.progress = FALSE

# MANDANTORY: calling the setup script also DO NOT change this line
source(file.path(envimaR::alternativeEnvi(root_folder = root_folder),"src/000-rspatial-setup.R"),local = knitr::knit_global())


# 1 - start script
#-----------------------------
#--- create path link to the original las file
if (!file.exists(paste0(envrmt$path_tmp,"/chm.zip")))
utils::download.file(url="https://github.com/gisma/gismaData/raw/master/uavRst/data/lidR_data.zip",
                     destfile=paste0(envrmt$path_tmp,"/chm.zip"))
unzip(paste0(envrmt$path_tmp,"/chm.zip"),
      exdir = envrmt$path_tmp,  
      overwrite = TRUE)

las_files = list.files(envrmt$path_tmp,
                       pattern = glob2rx("*.las"),
                       full.names = TRUE)

#--- create CHM with lidR catalog
#- source: https://github.com/Jean-Romain/lidR/wiki/Rasterizing-perfect-canopy-height-models
future::plan(future::multisession)
ctg <- lidR::readLAScatalog(las_files[[1]])
lidR::projection(ctg) <- 25832
lidR::opt_chunk_size(ctg) = 100
lidR::opt_chunk_buffer(ctg) <- 5
lidR::opt_progress(ctg) <- FALSE
lidR::opt_laz_compression(ctg) <- TRUE
ctg@output_options$drivers$Raster$param$overwrite <- TRUE

#--- height nomalisaion within the point cloud

lidR::opt_output_files(ctg) <- paste0(envrmt$path_l_norm,"/{ID}","_norm_height")
norm_ctg <- lidR::normalize_height(ctg,lidR::tin())
# calculate chm
chm <- grid_canopy(norm_ctg, res = 1.0, lidR::pitfree(thresholds = c(0,2,5,10,15), c(0,1.5)))
chm = get_vrt_img("chm",envrmt$path_l_norm,"_norm_height")
```

### Check the catalog
```{r check-ctg,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}
# check new ctg
las_check(norm_ctg)
```

### The visualization of an operating lidR catalog action.

The `ctg` catalog  shows the extent of the original las file as provided by the the data server. The vector that is visualized is the resulting `lidR` catalog containing all extracted parameters. Same with the `ctg` here you see the extracted maximum Altitude of each tile used for visualization. Feel free to investigate the catalog parameters by clicking on the tiles.  


```{r vis-ctg,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}
#--- plots
mapview::mapview(norm_ctg, zcol="Max.Z")
```




## Let's plot the results

### mapview 
```{r mapview,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}
#- mapview plot
mapview::mapview(chm[[1]],layer.name = "pitfree chm 1 m² cells height [m]",col.regions=lidR::height.colors(20))
```



### Classical plot 
```{r classic_plot,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}

#- classic plot
plot( chm,
      col = lidR::height.colors(20),
      main = "pitfree chm 1 m² cells",
      cex.main = 1) 
```

### tmap plot 
```{r tmap,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}

#- tmap plot
tmap::tm_shape(chm) +
tmap::tm_raster( title = "pitfree chm 1 m² cells", palette = lidR::height.colors(20)) +
  tm_grid()+
  tm_layout(legend.outside = TRUE)
```

### ggplot 
```{r ggplot,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}

#- ggplot plot with stars
ggplot2::ggplot() + 
stars::geom_stars(data = stars::st_as_stars(chm)) + 
ggplot2::scale_fill_gradientn(colours=lidR::height.colors(20)) +
ggplot2::coord_equal()+
ggplot2::guides(fill=guide_legend(title="pitfree chm 1 m² cells"))
  
```



[Download Script](https://gist.github.com/gisma/4172ef049b116abb1454233c8950d587)

## The real challenge

Assuming the above script (or the scripts you have adapted) runs without any technical errors, the exciting part of the data preprocessing comes now. In addition we ave to cross check and answer a lot of questions:

- Which algorithms did I use and why? 
- 45m high trees - does that make sense? 
- If not why not? 
- What correction options do I have and why? 
- Is the error due to the data, the algorithms, the script or all together?
- ...

## Where to go?

To answer this questions we need a two folded approach. 
- First we need technically to strip down the script to a *real* control script, all functionality that is used more than once will be moved into *functions* or other static scripts. 
- Second from a scientific point of view we need to find out what algorithm is most suitable and reliable to answer the question of calculating a set of paramters for deriving structural forest index classes. This also includes that we have to decide which classes and why... .



## Ressources

- You find all related materials at the [aGIS Ressource](https://gisma-courses.github.io/gi-modules/post/2021-11-16-agis-ressourcen/) post.


## Comments & Suggestions  

