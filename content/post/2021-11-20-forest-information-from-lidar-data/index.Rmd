---
title: Forest Information from LiDAR data
author: Chris Reudenbach
date: '2021-11-20'
slug: []
categories: ["modules"]
tags: ["lidar","remote sensing","forest","ecology"]
description: ''
image: '/assets/images/forest-sp.jpg'
toc: true
output:
  blogdown::html_page:
    keep_md: yes

weight: 101

---
## How to get started?


For this first example we take a typical situation:
- we have no idea about the software and possible solutions
- we have no idea about LiDAR data processing and analysis
- we just google something like *lidR package tutorial*

Among the top ten is [The lidR package book](https://jean-romain.github.io/lidRbook/) . So let's follow the white rabbit...

## Creating a Canopy Height Model (CHM) reloaded

After successful application of the tutorial we will transfer it into a suitable script for our workflow. Please check the comments for better understanding and do it **step by step!** again. Please note that the following script is meant to be an basic example how: 
- to organize scripts in common 
- with a hands on example for creating a canopy height model.


After revisiting the tutorial is seems to be a good choice to follow the tutorial of the `lidR` developer that is  [Digital Surface Model and Canopy Height model](https://jean-romain.github.io/lidRbook/chm.html)  in the above tutorial. Why? because for the first Jean-Romain Roussel explains 6 ways how to create in a very simple approach a CHM, second to show up that it makes sense to read and third to loop back because it does not work with bigger files. Let us start with the new script structure.


## Basic Script to derive a Canopy Height Model



<script src="https://gist.github.com/gisma/89881e23f5c2da91d161a9668386b715.js"></script>


      


<iframe src="/gi-modules/assets/misc/chm_one_tile.html" width="700" height="680" frameborder="0" allowfullscreen="allowfullscreen" allow="geolocation *; microphone *; camera *; midi *; encrypted-media *" ></iframe>
<figcaption>*Canopy Height Model map*</figcaption>

## lidR catalog - coping with computer memory resources

The above example is based on a las-tile of 250 by 250 meter. That means a fairly **small** tile size. If you did not run in memory or CPU problems you can deal with these tiles by simple looping.  

**But** you have to keep in mind that even if a tile based processing can easily be handled with loops but has some pitfalls. E.g. if you compute some focal operations, you have to make sure that if the filter is close to the boundary of the tile that data from the neighboring tile is loaded in addition. Also the`las` tile size may be a problem for your memory availability. 

The `lidR` package comes with a feature called catalog and a set of catalog functions which make tile-based life easier. A catalog meta object also holds information on e.g. the cartographic reference system of the data or the cores used for parallel processing. So we start over again - this time with a **real** data set.

Please note that dealing with `lidR` catalogs is pretty stressful for the memory administration of your `rsession`. So best practices is to **Restart your Rsesseion and clean up your environment**. This will help you to avoid frustrating situation like restarting your PC after hours of waiting...




### Using `lidR catalog` to derive a canopy height model

The following script will setup a `lidR catalog` and a more sophisticated approach to calculate the DEM, DSM and CHM. Please note that you have to exchange the code snippet dealing with the LiDAR data, If not already done download the course related [LiDAR data set of the MOF area](http://gofile.me/3Z8AJ/omf5DvyhL) (VPN required) . Store the the data to the e.g. `envrmt$path_l_raw` folder. 

#### VRT workaround

```{r echo=FALSE} 

blogdown::shortcode("collapse", "Workaround for the IO error problem with the lidR catalog",
                    "Please download and copy the [get_vrt_img.R](https://gist.github.com/gisma/5127a360728666eddb96807ad37bb475) script into the subfolder *functions* of your project src folder.Depending on your installation it might be necessary to work around an unexpected behaviour with the returnd vrt files")
```

#### Complete catalog script
<p>
  <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse_json" aria-expanded="false" aria-controls="collapse_json">
    Show complete script for `lidR catalog`
  </button>
<div class="collapse" id="collapse_json">
  <div class="card card-body">
<script src="https://gist.github.com/gisma/4172ef049b116abb1454233c8950d587.js"></script>


 </div>
</div>


```{r warmup, include= FALSE,echo=FALSE, error=FALSE,message=FALSE, warning=FALSE, cache=FALSE, include=FALSE}
knitr::knit_global()
require(envimaR)

# MANDANTORY: defining the root folder DO NOT change this line
root_folder = "~/edu/agis"

#-- Further customization of the setup by the user this section
#-- can be freely customized only the definition of additional packages
#-- and directory paths MUST be done using the two variables
#-- appendpackagesToLoad and appendProjectDirList
#-- feel free to remove this lines if you do not need them
# define  additional packages comment if not needed
appendpackagesToLoad = c("lidR","future","lwgeom")
# define additional subfolders comment if not needed
appendProjectDirList =  c("data/lidar/","data/lidar/l_raster/","data/lidar/l_raw/","data/lidar/l_norm/")

## define current projection (It is not magic you need to check the meta data or ask your instructor)
## ETRS89 / UTM zone 32N
proj4 = "+proj=utm +zone=32 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
epsg_number = 25832

lidR.progress = FALSE

# MANDANTORY: calling the setup script also DO NOT change this line
source(file.path(envimaR::alternativeEnvi(root_folder = root_folder),"src/000-rspatial-setup.R"),local = knitr::knit_global())


# 1 - start script
#-----------------------------
#--- create path link to the original las file
if (!file.exists(paste0(envrmt$path_tmp,"/chm.zip")))
utils::download.file(url="https://github.com/gisma/gismaData/raw/master/uavRst/data/lidR_data.zip",
                     destfile=paste0(envrmt$path_tmp,"/chm.zip"))
unzip(paste0(envrmt$path_tmp,"/chm.zip"),
      exdir = envrmt$path_tmp,  
      overwrite = TRUE)

las_files = list.files(envrmt$path_tmp,
                       pattern = glob2rx("*.las"),
                       full.names = TRUE)#### VRT workaround

#--- create CHM with lidR catalog
#- source: https://github.com/Jean-Romain/lidR/wiki/Rasterizing-perfect-canopy-height-models
future::plan(future::multisession)
ctg <- lidR::readLAScatalog(las_files[[1]])
lidR::opt_chunk_size(ctg) = 100
lidR::opt_chunk_buffer(ctg) <- 5
lidR::opt_progress(ctg) <- FALSE
lidR::opt_laz_compression(ctg) <- TRUE
ctg@output_options$drivers$Raster$param$overwrite <- TRUE

#--- height normalization within the point cloud

lidR::opt_output_files(ctg) <- paste0(envrmt$path_l_norm,"/{ID}","_norm_height")
norm_ctg <- lidR::normalize_height(ctg,lidR::tin())
# calculate chm
chm <- grid_canopy(norm_ctg, res = 1.0, lidR::pitfree(thresholds = c(0,2,5,10,15), c(0,1.5)))
chm = get_vrt_img("chm",envrmt$path_l_norm,"_norm_height")
chm=raster(chm)
crs(chm) <- 25832
#writeRaster(chm,paste0(envrmt$path_l_raster,"chm.tif"))

```

### Check the catalog
```{r check-ctg,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}
# check new ctg
las_check(norm_ctg)
```

### The visualization of an operating lidR catalog action.

The `ctg` catalog  shows the extent of the original `las` file as provided by the the data server. The vector that is visualized is the resulting `lidR` catalog containing all extracted parameters. Same with the `ctg` here you see the extracted maximum Altitude of each tile used for visualization. Feel free to investigate the catalog parameters by clicking on the tiles.  



```{r vis-ctg,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}
#--- plots
mapview(chm, zcol="Max.Z",col.region=viridis::viridis(16))
```




## Let's plot the results
You will find below some very common approaches to visualize the results as a map. Please note this are just very basic maps for getting a first impression of the data.

### mapview 
<div class="fold s ">
```{r mapview,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}
#- mapview plot
mapview::mapview(chm,layer.name = "pitfree chm 1sqm tree height [m]",col.regions=lidR::height.colors(20))
```
</div>


### Base plot 
<div class="fold s ">
```{r classic_plot,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}

#- classic plot
plot( chm,
      col = lidR::height.colors(20),
      main = "pitfree chm 1 m² cells",
      cex.main = 1) 
```
</div>
### tmap plot 

<div class="fold s ">
```{r tmap,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}

#- tmap plot
tmap::tm_shape(chm) +
tmap::tm_raster( title = "pitfree chm 1 m² cells", palette = lidR::height.colors(20)) +
  tm_grid()+
  tm_layout(legend.outside = TRUE)
```
</div>
### ggplot 

<div class="fold s ">
```{r ggplot,  message=FALSE, warning=FALSE, cache=FALSE, render=TRUE}

#- ggplot plot with stars
ggplot2::ggplot() + 
stars::geom_stars(data = stars::st_as_stars(chm)) + 
ggplot2::scale_fill_gradientn(colours=lidR::height.colors(20)) +
ggplot2::coord_equal()+
ggplot2::guides(fill=guide_legend(title="pitfree chm 1 m² cells"))
  
```

</div>

[Download Script](https://gist.github.com/gisma/4172ef049b116abb1454233c8950d587)

## The real challenge

Assuming the above script (or the scripts you have adapted) runs without any technical errors, the exciting part of the data preprocessing comes now. In addition we ave to cross check and answer a lot of questions:

- Which algorithms did I use and why? 
- 45m high trees - does that make sense? 
- If not why not? 
- What correction options do I have and why? 
- Is the error due to the data, the algorithms, the script or all together?
- ...

## Where to go?

To answer this questions we need a two folded approach. 
- First we need technically to strip down the script to a *real* control script, all functionality that is used more than once will be moved into *functions* or other static scripts. 
- Second from a scientific point of view we need to find out what algorithm is most suitable and reliable to answer the question of calculating a set of paramters for deriving structural forest index classes. This also includes that we have to decide which classes and why... .



## Ressources

- You find all related materials at the [aGIS Ressource](https://gisma-courses.github.io/gi-modules/post/2021-11-16-agis-ressourcen/) post.


## Comments & Suggestions  

